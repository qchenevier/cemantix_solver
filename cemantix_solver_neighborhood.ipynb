{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import urllib.request\n","from time import sleep\n","\n","import ipywidgets as widgets\n","import pandas as pd\n","import requests\n","from gensim.models import KeyedVectors\n","from IPython.display import display\n","from tqdm import tqdm\n","import sys\n","import numpy as np\n","\n","\n","class DownloadProgressBar(tqdm):\n","    def update_to(self, b=1, bsize=1, tsize=None):\n","        if tsize is not None:\n","            self.total = tsize\n","        self.update(b * bsize - self.n)\n","\n","\n","def download_file(url, filename):\n","    with output_download:\n","        with DownloadProgressBar(\n","            unit=\"B\", unit_scale=True, miniters=1, desc=filename, file=sys.stdout\n","        ) as t:\n","            urllib.request.urlretrieve(\n","                url, filename=filename, reporthook=t.update_to\n","            )\n","\n","\n","def download_if_not_present(url):\n","    filename = url.split(\"/\")[-1]\n","    if not os.path.exists(filename):\n","        download_file(url, filename)\n","        with output_download:\n","            print(\"W2V file downloaded.\")\n","    else:\n","        with output_download:\n","            print(\"W2V file already present.\")\n","    return filename\n","\n","\n","def get_score(word):\n","    url = \"https://cemantix.herokuapp.com/score\"\n","    response = requests.post(url, {\"word\": word})\n","    sleep(1)  # API may refuse too fast requests\n","    return response.json().get(\"score\", None)\n","\n","\n","def get_score_from_word_key(word_key, cache):\n","    if word_key in cache:\n","        return cache[word_key]\n","    word = word_key.split(\"_\")[0].lower()\n","    score = get_score(word)\n","    cache[word_key] = score\n","    return score\n","\n","\n","def add_word_to_scores(word_key, word_score, scores, model):\n","    if word_key not in scores.key.values:\n","        scores = pd.concat(\n","            [\n","                scores,\n","                pd.DataFrame.from_records(\n","                    [\n","                        {\n","                            \"key\": word_key,\n","                            \"score\": word_score,\n","                            \"vector\": model.get_vector(word_key),\n","                        }\n","                    ]\n","                ),\n","            ],\n","            axis=0,\n","        )\n","    return scores.sort_values(by=\"score\", ascending=False).reset_index(\n","        drop=True\n","    )\n","\n","\n","def add_random_word_to_scores(scores, vocab, model, cache):\n","    sample_score = None\n","    while sample_score is None:\n","        sample = vocab.sample(1).iloc[0]\n","        sample_score = get_score_from_word_key(sample.key, cache)\n","    return add_word_to_scores(sample.key, sample_score, scores, model)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["url = \"https://embeddings.net/embeddings/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin\"\n","output_download = widgets.Output()\n","display(output_download)\n","embeddings_filename = download_if_not_present(url)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = KeyedVectors.load_word2vec_format(\n","    embeddings_filename,\n","    binary=True,\n","    unicode_errors=\"ignore\",\n",")\n","vocab = pd.DataFrame({\"key\": model.index_to_key}).assign(\n","    word=lambda df: df.key.str.split(\"_\").str[0].str.lower()\n",")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["widget_N_vocab = widgets.FloatLogSlider(value=3000, min=0, max=np.log10(vocab.shape[0]))\n","widget_N_neighborhood = widgets.IntSlider(value=300, min=0, max=1000)\n","display(widgets.HBox([widgets.Label(\"Vocabulary size (log scale):\"), widget_N_vocab]))\n","display(widgets.HBox([widgets.Label(\"Neighborhood search size:\"), widget_N_neighborhood]))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def search(button):\n","    N_vocab = int(widget_N_vocab.value)\n","    N_neighborhood = widget_N_neighborhood.value\n","    vocab_selection = vocab.head(N_vocab)\n","    \n","    with output:\n","        print(f\"Search start.\")\n","    cache = dict()\n","    scores = pd.DataFrame(columns=[\"key\", \"score\", \"vector\"])\n","    scores = add_random_word_to_scores(scores, vocab_selection, model, cache)\n","\n","    score = 0\n","    while score < 1:\n","        key, score, vector = scores.iloc[0].tolist()\n","        neighborhood = [\n","            w\n","            for w, _ in model.most_similar(\n","                key, restrict_vocab=N_vocab, topn=N_neighborhood\n","            )\n","        ]\n","        for new_key in neighborhood:\n","            new_score = get_score_from_word_key(new_key, cache)\n","            if new_score is not None and new_score > score:\n","                with output:\n","                    print(f\"{len(cache)} - {new_key}: {new_score}\")\n","                scores = add_word_to_scores(new_key, new_score, scores, model)\n","                break\n","        if new_score is None or new_score <= score:\n","            raise Exception(\n","                \"Neighborhood explored without finding new best option. Please increase N_neighborhood\"\n","            )\n","        if new_score == 1:\n","            with output:\n","                print(f\"Finished in {len(cache)} requests. The solution is '{new_key}'.\")\n","            return new_key\n","\n","\n","button = widgets.Button(description=\"Search\")\n","output = widgets.Output()\n","display(button, output)\n","button.on_click(search)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}